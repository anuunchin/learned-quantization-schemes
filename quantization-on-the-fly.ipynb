{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuunchinbat/Desktop/Thesis/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784) 128\n",
      "(None, 128) 10\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 69.5257 - accuracy: 0.1801 - val_loss: 1.9165 - val_accuracy: 0.2680\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7724 - accuracy: 0.3112 - val_loss: 1.7119 - val_accuracy: 0.3556\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7022 - accuracy: 0.3246 - val_loss: 1.6856 - val_accuracy: 0.3119\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7631 - accuracy: 0.2846 - val_loss: 1.7010 - val_accuracy: 0.3015\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.8333 - accuracy: 0.2743 - val_loss: 1.9355 - val_accuracy: 0.2795\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.9580 - accuracy: 0.2511 - val_loss: 1.9572 - val_accuracy: 0.2572\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.0748 - accuracy: 0.2157 - val_loss: 2.2495 - val_accuracy: 0.1694\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.1159 - accuracy: 0.1873 - val_loss: 2.1596 - val_accuracy: 0.1734\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.1413 - accuracy: 0.1837 - val_loss: 2.1751 - val_accuracy: 0.1767\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.1525 - accuracy: 0.1769 - val_loss: 2.2099 - val_accuracy: 0.1780\n",
      "313/313 [==============================] - 0s 938us/step - loss: 2.2099 - accuracy: 0.1780\n",
      "Quantized Model Test Accuracy: 0.17800000309944153\n"
     ]
    }
   ],
   "source": [
    "#The scale and zero point parameteers are learned globally for the entire layer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "initializer_type = \"ones\"\n",
    "\n",
    "class LearnedQuantizedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LearnedQuantizedDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape, self.units)\n",
    "        self.w = self.add_weight(           shape=(input_shape[-1], self.units), #784x128 -> #128x10\n",
    "                                            initializer=initializer_type,\n",
    "                                            trainable=True)\n",
    "        self.b = self.add_weight(           shape=(self.units,),                    \n",
    "                                            initializer=initializer_type, \n",
    "                                            trainable=True)\n",
    "        self.scale = self.add_weight(       shape=(1,),                             \n",
    "                                            initializer=initializer_type, \n",
    "                                            trainable=True)\n",
    "        self.zero_point = self.add_weight(  shape=(1,),                            \n",
    "                                            initializer=initializer_type,\n",
    "                                            trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #during forward pass, the model uses quantized (rounded) weights to simulate the effect of quantization    \n",
    "        #during backpropagation Gradients are calculated as if the weights were not rounded,\n",
    "        #This prevents the non-differentiable rounding operation from disrupting the learning process.    \n",
    "        quantized_w = tf.stop_gradient(tf.round(self.w / self.scale + self.zero_point)) + \\\n",
    "                (self.w / self.scale + self.zero_point - tf.stop_gradient(self.w / self.scale + self.zero_point))\n",
    "        dequantized_w = (quantized_w - self.zero_point) * self.scale\n",
    "        quantized_b = tf.stop_gradient(tf.round(self.b / self.scale + self.zero_point)) + \\\n",
    "                (self.b / self.scale + self.zero_point - tf.stop_gradient(self.b / self.scale + self.zero_point))\n",
    "        dequantized_b = (quantized_b - self.zero_point) * self.scale\n",
    "\n",
    "        output = tf.matmul(inputs, dequantized_w) + dequantized_b\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "flatten_layer = Flatten()(input_layer)\n",
    "quantized_dense_layer = LearnedQuantizedDense(128, activation='relu')(flatten_layer)\n",
    "output_layer = LearnedQuantizedDense(10, activation='softmax')(quantized_dense_layer)\n",
    "\n",
    "quantized_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "quantized_model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "quantized_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = quantized_model.evaluate(x_test, y_test)\n",
    "print(f'Quantized Model Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuunchinbat/Desktop/Thesis/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 784) 128\n",
      "(None, 128) 10\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2563 - accuracy: 0.9245 - val_loss: 0.1831 - val_accuracy: 0.9482\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1636 - accuracy: 0.9550 - val_loss: 0.1691 - val_accuracy: 0.9549\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1566 - accuracy: 0.9589 - val_loss: 0.1944 - val_accuracy: 0.9522\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1387 - accuracy: 0.9643 - val_loss: 0.1663 - val_accuracy: 0.9603\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1283 - accuracy: 0.9680 - val_loss: 0.2029 - val_accuracy: 0.9528\n",
      "313/313 [==============================] - 0s 739us/step - loss: 0.2029 - accuracy: 0.9528\n",
      "Quantized Model Test Accuracy: 0.9527999758720398\n"
     ]
    }
   ],
   "source": [
    "#Scale and zero point are learned as vectors, instead of scalars\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# let's not normalize the inputs -> so that it the input quantization can be learned\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0 # this is essentially pre-qunatization -> there's already 255 buckets, we would go down, not up, so \n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "initializer_type = \"random_normal\"\n",
    "\n",
    "class LearnedQuantizedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LearnedQuantizedDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape, self.units)\n",
    "        self.w = self.add_weight(           shape=(input_shape[-1], self.units), \n",
    "                                            initializer=initializer_type,\n",
    "                                            trainable=True)\n",
    "        self.b = self.add_weight(           shape=(self.units,), \n",
    "                                            initializer=initializer_type, \n",
    "                                            trainable=True)\n",
    "        # This is good to go: scale value per hyperparameter\n",
    "        self.scale = self.add_weight(       shape=(input_shape[-1], 1), \n",
    "                                            initializer=initializer_type, \n",
    "                                            trainable=True)\n",
    "        self.zero_point = self.add_weight(  shape=(input_shape[-1], 1), \n",
    "                                            initializer=initializer_type,\n",
    "                                            trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "    # if you set the scale to a very small number, then the number of unique numbers is exactly the same.\n",
    "    # the larger the scale is the fewer quantized values -> this is what we want.\n",
    "    # pls take a lok into the scale -> most likely it's going to 0, which is n\n",
    "    # self.scale and self.zero_point already have the correct shape\n",
    "        quantized_w = tf.stop_gradient(tf.round(self.w / self.scale + self.zero_point)) + \\\n",
    "                    (self.w / self.scale + self.zero_point - tf.stop_gradient(self.w / self.scale + self.zero_point))\n",
    "        dequantized_w = (quantized_w - self.zero_point) * self.scale\n",
    "        quantized_b = tf.stop_gradient(tf.round(self.b / self.scale[0, 0] + self.zero_point[0, 0])) + \\\n",
    "                    (self.b / self.scale[0, 0] + self.zero_point[0, 0] - tf.stop_gradient(self.b / self.scale[0, 0] + self.zero_point[0, 0]))\n",
    "        dequantized_b = (quantized_b - self.zero_point[0, 0]) * self.scale[0, 0]\n",
    "\n",
    "    # other thing that would be cool to know is to on the inputs itself can we learn a quantization \n",
    "    # using the gradients, can we modify the input parameters and quantizise \n",
    "        output = tf.matmul(inputs, dequantized_w) + dequantized_b\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "input_layer = Input(shape=(28, 28, 1))\n",
    "flatten_layer = Flatten()(input_layer)\n",
    "quantized_dense_layer = LearnedQuantizedDense(128, activation='relu')(flatten_layer)\n",
    "output_layer = LearnedQuantizedDense(10, activation='softmax')(quantized_dense_layer)\n",
    "\n",
    "quantized_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "quantized_model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "quantized_model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "loss, accuracy = quantized_model.evaluate(x_test, y_test)\n",
    "print(f'Quantized Model Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
