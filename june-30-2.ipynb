{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "eps_float32 = np.finfo(np.float32).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleTrackingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, layer):\n",
    "        super(ScaleTrackingCallback, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.scale_values_per_epoch_w = []\n",
    "        self.average_scale_values_w = []\n",
    "        self.scale_values_per_epoch_b = []\n",
    "        self.average_scale_values_b = []\n",
    "\n",
    "        self.epoch_loss = []\n",
    "        self.epoch_accuracy = []\n",
    "        self.num_epochs = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        scale_values_w = self.layer.scale_w.numpy().flatten()\n",
    "        self.scale_values_per_epoch_w.append(scale_values_w)\n",
    "        average_scale_value_w = np.mean(scale_values_w)\n",
    "        self.average_scale_values_w.append(average_scale_value_w)\n",
    "\n",
    "        scale_values_b = self.layer.scale_b.numpy().flatten()\n",
    "        self.scale_values_per_epoch_b.append(scale_values_b)\n",
    "        average_scale_value_b = np.mean(scale_values_b)\n",
    "        self.average_scale_values_b.append(average_scale_value_b)\n",
    "\n",
    "        self.epoch_loss.append(logs['loss'])\n",
    "        self.epoch_accuracy.append(logs['accuracy'])\n",
    "        print(f\"\\nEpoch {epoch+1}: \\nAverage scale value of w: {average_scale_value_w}, \\nAverage scale value of b: {average_scale_value_b}, \\nLoss: {logs['loss']}, \\nAccuracy: {logs['accuracy']}\")\n",
    "\n",
    "    def plot_scale_values(self, layer_name):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot each scale value trajectory\n",
    "        for i in range(len(self.scale_values_per_epoch_w[0])):\n",
    "            scale_trajectory = [epoch[i] for epoch in self.scale_values_per_epoch_w]\n",
    "            plt.plot(range(1, len(self.scale_values_per_epoch_w) + 1), scale_trajectory, linestyle='-', marker='o')\n",
    "\n",
    "        # Plot the average scale values\n",
    "        #plt.plot(range(1, len(self.average_scale_values) + 1), self.average_scale_values, label='Average Scale Value', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Scale Values')\n",
    "        plt.title('Scale Values of w per Epoch - ' + layer_name)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_w) + 1))  \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot each scale value trajectory\n",
    "        for i in range(len(self.scale_values_per_epoch_b[0])):\n",
    "            scale_trajectory = [epoch[i] for epoch in self.scale_values_per_epoch_b]\n",
    "            plt.plot(range(1, len(self.scale_values_per_epoch_b) + 1), scale_trajectory, linestyle='-', marker='o')\n",
    "\n",
    "        # Plot the average scale values\n",
    "        #plt.plot(range(1, len(self.average_scale_values) + 1), self.average_scale_values, label='Average Scale Value', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Scale Values')\n",
    "        plt.title('Scale Values of b per Epoch - ' + layer_name)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_b) + 1))  \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_accuracy(self, penalty_rate):\n",
    "        epochs = range(1, len(self.epoch_loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, self.epoch_loss, 'b-', label='Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss per Epoch - with penalty rate = ' + penalty_rate)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_w) + 1))  \n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, self.epoch_accuracy, 'r-', label='Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy per Epoch')\n",
    "        plt.xticks(range(1, len(self.epoch_accuracy) + 1))  \n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = np.expand_dims(x_train, -1)  # CNNs typically expect input data to be 4D\n",
    "x_test = np.expand_dims(x_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER 0: <keras.src.engine.input_layer.InputLayer object at 0x3094bfc10>\n",
      "  - Input Shape: [(None, 28, 28, 1)]\n",
      "  - Output Shape: [(None, 28, 28, 1)]\n",
      "\n",
      "LAYER 1: <keras.src.layers.reshaping.flatten.Flatten object at 0x3094bf6d0>\n",
      "  - Input Shape: (None, 28, 28, 1)\n",
      "  - Output Shape: (None, 784)\n",
      "\n",
      "LAYER 2: <__main__.LearnedQuantizedDense object at 0x303668f10>\n",
      "  - Input Shape: (None, 784)\n",
      "  - Output Shape: (None, 128)\n",
      "  - Shape of scale_w: (1, 128)\n",
      "  - Shape of scale_b: (1,)\n",
      "\n",
      "LAYER 3: <__main__.LearnedQuantizedDense object at 0x30a93a370>\n",
      "  - Input Shape: (None, 128)\n",
      "  - Output Shape: (None, 10)\n",
      "  - Shape of scale_w: (1, 10)\n",
      "  - Shape of scale_b: (1,)\n",
      "Epoch 1/20\n",
      "1854/1875 [============================>.] - ETA: 0s - loss: 15.5816 - accuracy: 0.8382\n",
      "Epoch 1: \n",
      "Average scale value of w: 0.08646468818187714, \n",
      "Average scale value of b: 0.12168022990226746, \n",
      "Loss: 15.455668449401855, \n",
      "Accuracy: 0.8382499814033508\n",
      "\n",
      "Epoch 1: \n",
      "Average scale value of w: -0.001312996493652463, \n",
      "Average scale value of b: 0.1167672872543335, \n",
      "Loss: 15.455668449401855, \n",
      "Accuracy: 0.8382499814033508\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 15.4557 - accuracy: 0.8382 - val_loss: 4.3025 - val_accuracy: 0.8566\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 3.0152 - accuracy: 0.8344\n",
      "Epoch 2: \n",
      "Average scale value of w: 0.09676329791545868, \n",
      "Average scale value of b: 0.14239537715911865, \n",
      "Loss: 3.015185832977295, \n",
      "Accuracy: 0.8343999981880188\n",
      "\n",
      "Epoch 2: \n",
      "Average scale value of w: -0.004800930619239807, \n",
      "Average scale value of b: 0.1500483751296997, \n",
      "Loss: 3.015185832977295, \n",
      "Accuracy: 0.8343999981880188\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.0152 - accuracy: 0.8344 - val_loss: 2.1848 - val_accuracy: 0.7812\n",
      "Epoch 3/20\n",
      "1870/1875 [============================>.] - ETA: 0s - loss: 1.8575 - accuracy: 0.7412\n",
      "Epoch 3: \n",
      "Average scale value of w: 0.11051954329013824, \n",
      "Average scale value of b: 0.172305166721344, \n",
      "Loss: 1.8572975397109985, \n",
      "Accuracy: 0.7408000230789185\n",
      "\n",
      "Epoch 3: \n",
      "Average scale value of w: -0.0032485511619597673, \n",
      "Average scale value of b: 0.19937404990196228, \n",
      "Loss: 1.8572975397109985, \n",
      "Accuracy: 0.7408000230789185\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.8573 - accuracy: 0.7408 - val_loss: 1.6908 - val_accuracy: 0.6296\n",
      "Epoch 4/20\n",
      " 625/1875 [=========>....................] - ETA: 2s - loss: 1.6315 - accuracy: 0.6612"
     ]
    }
   ],
   "source": [
    "\n",
    "class LearnedQuantizedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LearnedQuantizedDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"random_normal\", trainable=True)\n",
    "        self.scale_w = self.add_weight(shape=(1, self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.scale_b = self.add_weight(shape=(1,), initializer=\"ones\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        quantized_w = tf.stop_gradient(tf.floor(self.w / self.scale_w)) + self.w / self.scale_w - tf.stop_gradient(self.w / self.scale_w)\n",
    "        #quantized_w = tf.stop_gradient(tf.floor(self.w / self.scale_w))\n",
    "\n",
    "        dequantized_w = quantized_w * self.scale_w\n",
    "\n",
    "        quantized_b = tf.stop_gradient(tf.floor(self.b / self.scale_b)) + (self.b / self.scale_b - tf.stop_gradient(self.b / self.scale_b))\n",
    "        #quantized_b = tf.stop_gradient(tf.floor(self.b / self.scale_b))\n",
    "        \n",
    "        dequantized_b = quantized_b * self.scale_b\n",
    "\n",
    "        output = tf.matmul(inputs, dequantized_w) + dequantized_b\n",
    "#        output = tf.matmul(inputs, quantized_w) + quantized_b\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_scale(self):\n",
    "        return self.scale_w, self.scale_b\n",
    "\n",
    "def custom_loss(y_true, y_pred, scales_of_layers, weights_and_biases, penalty_rate):\n",
    "    cross_entropy_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # small epsilon to avoid division by zero\n",
    "    epsilon = eps_float32\n",
    "    scale_penalty = 0\n",
    "    \n",
    "    for i, ((scale_w, scale_b), (w, b)) in enumerate(zip(scales_of_layers, weights_and_biases)):\n",
    "        max_w = tf.reduce_max(w, axis=0)\n",
    "        min_w = tf.reduce_min(w, axis=0)\n",
    "        max_b = tf.reduce_max(b)\n",
    "        min_b = tf.reduce_min(b)\n",
    "        \n",
    "        max_w_quantized = (max_w / (scale_w + epsilon))\n",
    "        min_w_quantized = (min_w / (scale_w + epsilon))\n",
    "        max_b_quantized = (max_b / (scale_b + epsilon))\n",
    "        min_b_quantized = (min_b / (scale_b + epsilon))\n",
    "\n",
    "        range_of_quant_w = max_w_quantized - min_w_quantized\n",
    "        range_of_quant_b = max_b_quantized - min_b_quantized\n",
    "\n",
    "        scale_penalty += tf.reduce_mean(tf.abs(range_of_quant_w))\n",
    "        scale_penalty += tf.reduce_mean(tf.abs(range_of_quant_b))\n",
    "    \n",
    "    total_loss = cross_entropy_loss + penalty_rate * scale_penalty\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "flatten_layer = tf.keras.layers.Flatten()(input_layer)\n",
    "quantized_dense_layer1 = LearnedQuantizedDense(128, activation='relu')\n",
    "dense_output1 = quantized_dense_layer1(flatten_layer)\n",
    "quantized_dense_layer2 = LearnedQuantizedDense(10, activation='softmax')\n",
    "output_layer = quantized_dense_layer2(dense_output1)\n",
    "\n",
    "quantized_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Print layer details\n",
    "for i, layer in enumerate(quantized_model.layers):\n",
    "    print(f\"\\nLAYER {i}: {layer}\")\n",
    "    print(f\"  - Input Shape: {layer.input_shape}\")\n",
    "    print(f\"  - Output Shape: {layer.output_shape}\")\n",
    "    if hasattr(layer, 'get_scale'):\n",
    "        print(f\"  - Shape of scale_w: {layer.get_scale()[0].shape}\")\n",
    "        print(f\"  - Shape of scale_b: {layer.get_scale()[1].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "penalty_rates = [#0.0, \n",
    "                 #0.1, \n",
    "                 1.0, \n",
    "                 #10.0\n",
    "                ]\n",
    "\n",
    "for penalty_rate in penalty_rates:\n",
    "    quantized_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, \n",
    "                                                [quantized_model.get_layer(index=2).get_scale(), \n",
    "                                                quantized_model.get_layer(index=3).get_scale()],\n",
    "                                                [(quantized_model.get_layer(index=2).w, quantized_model.get_layer(index=2).b),\n",
    "                                                (quantized_model.get_layer(index=3).w, quantized_model.get_layer(index=3).b)],\n",
    "                                                penalty_rate=penalty_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    scale_tracking_callback_input_layer = ScaleTrackingCallback(quantized_model.get_layer(index=2))\n",
    "    scale_tracking_callback_dense_layer = ScaleTrackingCallback(quantized_model.get_layer(index=3))\n",
    "\n",
    "    quantized_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[scale_tracking_callback_input_layer,\n",
    "                scale_tracking_callback_dense_layer]\n",
    "    )\n",
    "\n",
    "    loss, accuracy = quantized_model.evaluate(x_test, y_test)\n",
    "    print(f'Quantized Model Test Accuracy: {accuracy}')\n",
    "\n",
    "    scale_tracking_callback_input_layer.plot_scale_values(layer_name=\"Quantized Dense Layer 1\")\n",
    "    scale_tracking_callback_dense_layer.plot_scale_values(layer_name=\"Quantized Dense Layer 2\")\n",
    "    scale_tracking_callback_dense_layer.plot_loss_accuracy(penalty_rate=str(penalty_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
