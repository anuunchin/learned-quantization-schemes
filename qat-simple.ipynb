{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuunchinbat/Desktop/Thesis/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32') \n",
    "x_test = x_test.astype('float32')\n",
    "x_train = np.expand_dims(x_train, -1) # CNNs typically expect input data to be 4D\n",
    "x_test = np.expand_dims(x_test, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(QuantizedDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"random_normal\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Simulate quantization with fake quantization\n",
    "        quantized_w = tf.quantization.fake_quant_with_min_max_args(self.w, min=-1.0, max=1.0, num_bits=8)\n",
    "        quantized_b = tf.quantization.fake_quant_with_min_max_args(self.b, min=-1.0, max=1.0, num_bits=8)\n",
    "        output = tf.matmul(inputs, quantized_w) + quantized_b\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(28, 28, 1))\n",
    "flatten_layer = Flatten()(input_layer)\n",
    "quantized_dense_layer_1 = QuantizedDense(128, activation='relu')(flatten_layer)\n",
    "output_layer = QuantizedDense(10, activation='softmax')(quantized_dense_layer_1)\n",
    "\n",
    "quantized_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "quantized_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                        loss='sparse_categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.9198 - accuracy: 0.8849 - val_loss: 0.3602 - val_accuracy: 0.9181\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2591 - accuracy: 0.9341 - val_loss: 0.2811 - val_accuracy: 0.9298\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2412 - accuracy: 0.9397 - val_loss: 0.2842 - val_accuracy: 0.9390\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2283 - accuracy: 0.9467 - val_loss: 0.2909 - val_accuracy: 0.9410\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2103 - accuracy: 0.9505 - val_loss: 0.2818 - val_accuracy: 0.9452\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2073 - accuracy: 0.9541 - val_loss: 0.3133 - val_accuracy: 0.9379\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1921 - accuracy: 0.9557 - val_loss: 0.2791 - val_accuracy: 0.9480\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2015 - accuracy: 0.9559 - val_loss: 0.2892 - val_accuracy: 0.9459\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1817 - accuracy: 0.9597 - val_loss: 0.3162 - val_accuracy: 0.9399\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1920 - accuracy: 0.9592 - val_loss: 0.3228 - val_accuracy: 0.9483\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1722 - accuracy: 0.9624 - val_loss: 0.3320 - val_accuracy: 0.9527\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1730 - accuracy: 0.9628 - val_loss: 0.3199 - val_accuracy: 0.9524\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1648 - accuracy: 0.9658 - val_loss: 0.4463 - val_accuracy: 0.9523\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1643 - accuracy: 0.9647 - val_loss: 0.3463 - val_accuracy: 0.9535\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1577 - accuracy: 0.9659 - val_loss: 0.3839 - val_accuracy: 0.9484\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1637 - accuracy: 0.9657 - val_loss: 0.3519 - val_accuracy: 0.9523\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1566 - accuracy: 0.9660 - val_loss: 0.3451 - val_accuracy: 0.9506\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1514 - accuracy: 0.9669 - val_loss: 0.3453 - val_accuracy: 0.9505\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1462 - accuracy: 0.9675 - val_loss: 0.4041 - val_accuracy: 0.9555\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.1566 - accuracy: 0.9672 - val_loss: 0.4082 - val_accuracy: 0.9509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1314ca160>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model.fit(x_train, y_train, epochs=20, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = quantized_model.evaluate(x_test, y_test)\n",
    "print(f'Quantized Model Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
