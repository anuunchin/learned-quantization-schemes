{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuunchinbat/Desktop/Thesis/myenv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER 0: <keras.src.engine.input_layer.InputLayer object at 0x105aaa5b0>\n",
      "  - Input Shape: [(None, 28, 28, 1)]\n",
      "  - Output Shape: [(None, 28, 28, 1)]\n",
      "\n",
      "LAYER 1: <keras.src.layers.reshaping.flatten.Flatten object at 0x1756fc9d0>\n",
      "  - Input Shape: (None, 28, 28, 1)\n",
      "  - Output Shape: (None, 784)\n",
      "\n",
      "LAYER 2: <__main__.LearnedQuantizedDense object at 0x17ae33610>\n",
      "  - Input Shape: (None, 784)\n",
      "  - Output Shape: (None, 128)\n",
      "  - Shape of scale_w: (784, 1)\n",
      "  - Shape of scale_b: (1,)\n",
      "\n",
      "LAYER 3: <__main__.LearnedQuantizedDense object at 0x1780cee20>\n",
      "  - Input Shape: (None, 128)\n",
      "  - Output Shape: (None, 10)\n",
      "  - Shape of scale_w: (128, 1)\n",
      "  - Shape of scale_b: (1,)\n",
      "Epoch 1/20\n",
      "TESTING (784,) (784, 1)\n",
      "TESTING (128,) (128, 1)\n",
      "TESTING (784,) (784, 1)\n",
      "TESTING (128,) (128, 1)\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 3.0573 - accuracy: 0.7935TESTING (784,) (784, 1)\n",
      "TESTING (128,) (128, 1)\n",
      "\n",
      "Epoch 1: \n",
      "Average scale value of w: 0.00815226323902607, \n",
      "Average scale value of b: 1.283746600151062, \n",
      "Loss: 3.057300329208374, \n",
      "Accuracy: 0.7935000061988831\n",
      "\n",
      "Epoch 1: \n",
      "Average scale value of w: -0.005863654427230358, \n",
      "Average scale value of b: 1.0791035890579224, \n",
      "Loss: 3.057300329208374, \n",
      "Accuracy: 0.7935000061988831\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 3.0573 - accuracy: 0.7935 - val_loss: 1.1056 - val_accuracy: 0.8880\n",
      "Epoch 2/20\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.9752 - accuracy: 0.8806\n",
      "Epoch 2: \n",
      "Average scale value of w: 0.009802919812500477, \n",
      "Average scale value of b: 1.429154396057129, \n",
      "Loss: 0.9751084446907043, \n",
      "Accuracy: 0.880649983882904\n",
      "\n",
      "Epoch 2: \n",
      "Average scale value of w: -0.007092646788805723, \n",
      "Average scale value of b: 1.1985626220703125, \n",
      "Loss: 0.9751084446907043, \n",
      "Accuracy: 0.880649983882904\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.9751 - accuracy: 0.8806 - val_loss: 0.8951 - val_accuracy: 0.8751\n",
      "Epoch 3/20\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.8411 - accuracy: 0.8767\n",
      "Epoch 3: \n",
      "Average scale value of w: 0.008994832634925842, \n",
      "Average scale value of b: 1.6547127962112427, \n",
      "Loss: 0.8409725427627563, \n",
      "Accuracy: 0.8767499923706055\n",
      "\n",
      "Epoch 3: \n",
      "Average scale value of w: -0.00798037275671959, \n",
      "Average scale value of b: 1.419646143913269, \n",
      "Loss: 0.8409725427627563, \n",
      "Accuracy: 0.8767499923706055\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.8410 - accuracy: 0.8767 - val_loss: 0.7898 - val_accuracy: 0.8702\n",
      "Epoch 4/20\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.8146 - accuracy: 0.8694\n",
      "Epoch 4: \n",
      "Average scale value of w: 0.005339980125427246, \n",
      "Average scale value of b: 1.979288935661316, \n",
      "Loss: 0.8149417042732239, \n",
      "Accuracy: 0.8693000078201294\n",
      "\n",
      "Epoch 4: \n",
      "Average scale value of w: -0.010929735377430916, \n",
      "Average scale value of b: 1.7866430282592773, \n",
      "Loss: 0.8149417042732239, \n",
      "Accuracy: 0.8693000078201294\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.8149 - accuracy: 0.8693 - val_loss: 1.1007 - val_accuracy: 0.8337\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.7867 - accuracy: 0.8709\n",
      "Epoch 5: \n",
      "Average scale value of w: 0.002572477562353015, \n",
      "Average scale value of b: 2.3358378410339355, \n",
      "Loss: 0.7866796851158142, \n",
      "Accuracy: 0.8708999752998352\n",
      "\n",
      "Epoch 5: \n",
      "Average scale value of w: -0.010931268334388733, \n",
      "Average scale value of b: 2.313632011413574, \n",
      "Loss: 0.7866796851158142, \n",
      "Accuracy: 0.8708999752998352\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.7867 - accuracy: 0.8709 - val_loss: 0.7069 - val_accuracy: 0.8877\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.7857 - accuracy: 0.8690\n",
      "Epoch 6: \n",
      "Average scale value of w: -0.0037608512211591005, \n",
      "Average scale value of b: 2.641831398010254, \n",
      "Loss: 0.7857117056846619, \n",
      "Accuracy: 0.8690166473388672\n",
      "\n",
      "Epoch 6: \n",
      "Average scale value of w: -0.01500035636126995, \n",
      "Average scale value of b: 3.0193638801574707, \n",
      "Loss: 0.7857117056846619, \n",
      "Accuracy: 0.8690166473388672\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.7857 - accuracy: 0.8690 - val_loss: 0.8112 - val_accuracy: 0.8510\n",
      "Epoch 7/20\n",
      "1872/1875 [============================>.] - ETA: 0s - loss: 0.7840 - accuracy: 0.8672\n",
      "Epoch 7: \n",
      "Average scale value of w: -0.014063335955142975, \n",
      "Average scale value of b: 2.9108455181121826, \n",
      "Loss: 0.7845417261123657, \n",
      "Accuracy: 0.8670833110809326\n",
      "\n",
      "Epoch 7: \n",
      "Average scale value of w: -0.0200925525277853, \n",
      "Average scale value of b: 3.963785409927368, \n",
      "Loss: 0.7845417261123657, \n",
      "Accuracy: 0.8670833110809326\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.7845 - accuracy: 0.8671 - val_loss: 0.7503 - val_accuracy: 0.8766\n",
      "Epoch 8/20\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.7866 - accuracy: 0.8654\n",
      "Epoch 8: \n",
      "Average scale value of w: -0.02384105511009693, \n",
      "Average scale value of b: 3.1427931785583496, \n",
      "Loss: 0.7865067720413208, \n",
      "Accuracy: 0.8654500246047974\n",
      "\n",
      "Epoch 8: \n",
      "Average scale value of w: -0.02689024806022644, \n",
      "Average scale value of b: 5.092008113861084, \n",
      "Loss: 0.7865067720413208, \n",
      "Accuracy: 0.8654500246047974\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.7865 - accuracy: 0.8655 - val_loss: 0.7120 - val_accuracy: 0.8872\n",
      "Epoch 9/20\n",
      "1871/1875 [============================>.] - ETA: 0s - loss: 0.7771 - accuracy: 0.8691\n",
      "Epoch 9: \n",
      "Average scale value of w: -0.03334558382630348, \n",
      "Average scale value of b: 3.349496364593506, \n",
      "Loss: 0.7778657078742981, \n",
      "Accuracy: 0.8689500093460083\n",
      "\n",
      "Epoch 9: \n",
      "Average scale value of w: -0.03517932817339897, \n",
      "Average scale value of b: 6.425210952758789, \n",
      "Loss: 0.7778657078742981, \n",
      "Accuracy: 0.8689500093460083\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.7779 - accuracy: 0.8690 - val_loss: 0.9047 - val_accuracy: 0.8251\n",
      "Epoch 10/20\n",
      "1874/1875 [============================>.] - ETA: 0s - loss: 0.7875 - accuracy: 0.8657\n",
      "Epoch 10: \n",
      "Average scale value of w: -0.04529949277639389, \n",
      "Average scale value of b: 3.524890899658203, \n",
      "Loss: 0.7874007821083069, \n",
      "Accuracy: 0.8657333254814148\n",
      "\n",
      "Epoch 10: \n",
      "Average scale value of w: -0.046094171702861786, \n",
      "Average scale value of b: 7.83266019821167, \n",
      "Loss: 0.7874007821083069, \n",
      "Accuracy: 0.8657333254814148\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.7874 - accuracy: 0.8657 - val_loss: 0.7011 - val_accuracy: 0.8903\n",
      "Epoch 11/20\n",
      "1328/1875 [====================>.........] - ETA: 5s - loss: 1.2166 - accuracy: 0.8582"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 208\u001b[0m\n\u001b[1;32m    205\u001b[0m scale_tracking_callback_input_layer \u001b[38;5;241m=\u001b[39m ScaleTrackingCallback(quantized_model\u001b[38;5;241m.\u001b[39mget_layer(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    206\u001b[0m scale_tracking_callback_dense_layer \u001b[38;5;241m=\u001b[39m ScaleTrackingCallback(quantized_model\u001b[38;5;241m.\u001b[39mget_layer(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m--> 208\u001b[0m \u001b[43mquantized_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mscale_tracking_callback_input_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale_tracking_callback_dense_layer\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m quantized_model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantized Model Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "eps_float32 = np.finfo(np.float32).eps\n",
    "\n",
    "class ScaleTrackingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, layer):\n",
    "        super(ScaleTrackingCallback, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.scale_values_per_epoch_w = []\n",
    "        self.average_scale_values_w = []\n",
    "        self.scale_values_per_epoch_b = []\n",
    "        self.average_scale_values_b = []\n",
    "\n",
    "        self.epoch_loss = []\n",
    "        self.epoch_accuracy = []\n",
    "        self.num_epochs = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        scale_values_w = self.layer.scale_w.numpy().flatten()\n",
    "        self.scale_values_per_epoch_w.append(scale_values_w)\n",
    "        average_scale_value_w = np.mean(scale_values_w)\n",
    "        self.average_scale_values_w.append(average_scale_value_w)\n",
    "\n",
    "        scale_values_b = self.layer.scale_b.numpy().flatten()\n",
    "        self.scale_values_per_epoch_b.append(scale_values_b)\n",
    "        average_scale_value_b = np.mean(scale_values_b)\n",
    "        self.average_scale_values_b.append(average_scale_value_b)\n",
    "\n",
    "        self.epoch_loss.append(logs['loss'])\n",
    "        self.epoch_accuracy.append(logs['accuracy'])\n",
    "        print(f\"\\nEpoch {epoch+1}: \\nAverage scale value of w: {average_scale_value_w}, \\nAverage scale value of b: {average_scale_value_b}, \\nLoss: {logs['loss']}, \\nAccuracy: {logs['accuracy']}\")\n",
    "\n",
    "    def plot_scale_values(self, layer_name):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot each scale value trajectory\n",
    "        for i in range(len(self.scale_values_per_epoch_w[0])):\n",
    "            scale_trajectory = [epoch[i] for epoch in self.scale_values_per_epoch_w]\n",
    "            plt.plot(range(1, len(self.scale_values_per_epoch_w) + 1), scale_trajectory, linestyle='-', marker='o')\n",
    "\n",
    "        # Plot the average scale values\n",
    "        #plt.plot(range(1, len(self.average_scale_values) + 1), self.average_scale_values, label='Average Scale Value', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Scale Values')\n",
    "        plt.title('Scale Values of w per Epoch - ' + layer_name)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_w) + 1))  \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot each scale value trajectory\n",
    "        for i in range(len(self.scale_values_per_epoch_b[0])):\n",
    "            scale_trajectory = [epoch[i] for epoch in self.scale_values_per_epoch_b]\n",
    "            plt.plot(range(1, len(self.scale_values_per_epoch_b) + 1), scale_trajectory, linestyle='-', marker='o')\n",
    "\n",
    "        # Plot the average scale values\n",
    "        #plt.plot(range(1, len(self.average_scale_values) + 1), self.average_scale_values, label='Average Scale Value', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Scale Values')\n",
    "        plt.title('Scale Values of b per Epoch - ' + layer_name)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_b) + 1))  \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_accuracy(self, penalty_rate):\n",
    "        epochs = range(1, len(self.epoch_loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, self.epoch_loss, 'b-', label='Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss per Epoch - with penalty rate = ' + penalty_rate)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_w) + 1))  \n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, self.epoch_accuracy, 'r-', label='Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy per Epoch')\n",
    "        plt.xticks(range(1, len(self.epoch_accuracy) + 1))  \n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Prepare the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = np.expand_dims(x_train, -1)  # CNNs typically expect input data to be 4D\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "class LearnedQuantizedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LearnedQuantizedDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"random_normal\", trainable=True)\n",
    "        self.scale_w = self.add_weight(shape=(input_shape[-1], 1), initializer=\"random_normal\", trainable=True)\n",
    "        self.scale_b = self.add_weight(shape=(1,), initializer=\"ones\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        quantized_w = tf.stop_gradient(tf.floor(self.w / self.scale_w)) + self.w / self.scale_w - tf.stop_gradient(self.w / self.scale_w)\n",
    "        #quantized_w = tf.stop_gradient(tf.floor(self.w / self.scale_w))\n",
    "\n",
    "        dequantized_w = quantized_w * self.scale_w\n",
    "\n",
    "        quantized_b = tf.stop_gradient(tf.floor(self.b / self.scale_b)) + (self.b / self.scale_b - tf.stop_gradient(self.b / self.scale_b))\n",
    "        #quantized_b = tf.stop_gradient(tf.floor(self.b / self.scale_b))\n",
    "        \n",
    "        dequantized_b = quantized_b * self.scale_b\n",
    "        \n",
    "        output = tf.matmul(inputs, dequantized_w) + dequantized_b\n",
    "#        output = tf.matmul(inputs, quantized_w) + quantized_b\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_scale(self):\n",
    "        return self.scale_w, self.scale_b\n",
    "\n",
    "def custom_loss(y_true, y_pred, scales_of_layers, weights_and_biases, penalty_rate):\n",
    "    cross_entropy_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # small epsilon to avoid division by zero\n",
    "    epsilon = eps_float32\n",
    "    scale_penalty = 0\n",
    "    \n",
    "    for i, ((scale_w, scale_b), (w, b)) in enumerate(zip(scales_of_layers, weights_and_biases)):\n",
    "        max_w = tf.reduce_max(w, axis=1)\n",
    "        print(\"TESTING\", max_w.shape, scale_w.shape)\n",
    "        min_w = tf.reduce_min(w, axis=1)\n",
    "        max_b = tf.reduce_max(b)\n",
    "        min_b = tf.reduce_min(b)\n",
    "        \n",
    "        max_w_quantized = (max_w / (scale_w + epsilon))\n",
    "        min_w_quantized = (min_w / (scale_w + epsilon))\n",
    "        max_b_quantized = (max_b / (scale_b + epsilon))\n",
    "        min_b_quantized = (min_b / (scale_b + epsilon))\n",
    "\n",
    "        range_of_quant_w = max_w_quantized - min_w_quantized\n",
    "        range_of_quant_b = max_b_quantized - min_b_quantized\n",
    "\n",
    "        scale_penalty += tf.reduce_mean(tf.abs(range_of_quant_w))\n",
    "        scale_penalty += tf.reduce_mean(tf.abs(range_of_quant_b))\n",
    "    \n",
    "    total_loss = cross_entropy_loss + penalty_rate * scale_penalty\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "flatten_layer = tf.keras.layers.Flatten()(input_layer)\n",
    "quantized_dense_layer1 = LearnedQuantizedDense(128, activation='relu')\n",
    "dense_output1 = quantized_dense_layer1(flatten_layer)\n",
    "quantized_dense_layer2 = LearnedQuantizedDense(10, activation='softmax')\n",
    "output_layer = quantized_dense_layer2(dense_output1)\n",
    "\n",
    "quantized_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Print layer details\n",
    "for i, layer in enumerate(quantized_model.layers):\n",
    "    print(f\"\\nLAYER {i}: {layer}\")\n",
    "    print(f\"  - Input Shape: {layer.input_shape}\")\n",
    "    print(f\"  - Output Shape: {layer.output_shape}\")\n",
    "    if hasattr(layer, 'get_scale'):\n",
    "        print(f\"  - Shape of scale_w: {layer.get_scale()[0].shape}\")\n",
    "        print(f\"  - Shape of scale_b: {layer.get_scale()[1].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "penalty_rates = [#0.0, \n",
    "                 #0.1, \n",
    "                 1.0, \n",
    "                 #10.0\n",
    "                 ]\n",
    "\n",
    "for penalty_rate in penalty_rates:\n",
    "    quantized_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, \n",
    "                                                [quantized_model.get_layer(index=2).get_scale(), \n",
    "                                                quantized_model.get_layer(index=3).get_scale()],\n",
    "                                                [(quantized_model.get_layer(index=2).w, quantized_model.get_layer(index=2).b),\n",
    "                                                (quantized_model.get_layer(index=3).w, quantized_model.get_layer(index=3).b)],\n",
    "                                                penalty_rate=penalty_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    scale_tracking_callback_input_layer = ScaleTrackingCallback(quantized_model.get_layer(index=2))\n",
    "    scale_tracking_callback_dense_layer = ScaleTrackingCallback(quantized_model.get_layer(index=3))\n",
    "\n",
    "    quantized_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[scale_tracking_callback_input_layer,\n",
    "                scale_tracking_callback_dense_layer]\n",
    "    )\n",
    "\n",
    "    loss, accuracy = quantized_model.evaluate(x_test, y_test)\n",
    "    print(f'Quantized Model Test Accuracy: {accuracy}')\n",
    "\n",
    "    scale_tracking_callback_input_layer.plot_scale_values(layer_name=\"Quantized Dense Layer 1\")\n",
    "    scale_tracking_callback_dense_layer.plot_scale_values(layer_name=\"Quantized Dense Layer 2\")\n",
    "    scale_tracking_callback_dense_layer.plot_loss_accuracy(penalty_rate=str(penalty_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
