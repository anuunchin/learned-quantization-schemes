{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAYER 0: <keras.src.engine.input_layer.InputLayer object at 0x10620ec40>\n",
      "  - Input Shape: [(None, 28, 28, 1)]\n",
      "  - Output Shape: [(None, 28, 28, 1)]\n",
      "\n",
      "LAYER 1: <keras.src.layers.reshaping.flatten.Flatten object at 0x30dce79a0>\n",
      "  - Input Shape: (None, 28, 28, 1)\n",
      "  - Output Shape: (None, 784)\n",
      "\n",
      "LAYER 2: <__main__.LearnedQuantizedDense object at 0x31945b100>\n",
      "  - Input Shape: (None, 784)\n",
      "  - Output Shape: (None, 128)\n",
      "  - Shape of scale_w: (1, 128)\n",
      "  - Shape of scale_b: (1,)\n",
      "\n",
      "LAYER 3: <__main__.LearnedQuantizedDense object at 0x319467f10>\n",
      "  - Input Shape: (None, 128)\n",
      "  - Output Shape: (None, 10)\n",
      "  - Shape of scale_w: (1, 10)\n",
      "  - Shape of scale_b: (1,)\n",
      "Epoch 1/20\n",
      "1866/1875 [============================>.] - ETA: 0s - loss: 2.3237 - accuracy: 0.1036\n",
      "Epoch 1: \n",
      "Average scale value of w: 1.0, \n",
      "Average scale value of b: 1.0, \n",
      "Loss: 2.3236093521118164, \n",
      "Accuracy: 0.10373333096504211\n",
      "\n",
      "Epoch 1: \n",
      "Average scale value of w: 1.0, \n",
      "Average scale value of b: 0.09796600043773651, \n",
      "Loss: 2.3236093521118164, \n",
      "Accuracy: 0.10373333096504211\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 2.3236 - accuracy: 0.1037 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
      "Epoch 2/20\n",
      "1865/1875 [============================>.] - ETA: 0s - loss: 2.3015 - accuracy: 0.1125\n",
      "Epoch 2: \n",
      "Average scale value of w: 1.0, \n",
      "Average scale value of b: 1.0, \n",
      "Loss: 2.301506996154785, \n",
      "Accuracy: 0.11236666887998581\n",
      "\n",
      "Epoch 2: \n",
      "Average scale value of w: 1.0, \n",
      "Average scale value of b: 0.024017849937081337, \n",
      "Loss: 2.301506996154785, \n",
      "Accuracy: 0.11236666887998581\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 3/20\n",
      "1781/1875 [===========================>..] - ETA: 0s - loss: 2.3014 - accuracy: 0.1123"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 243\u001b[0m\n\u001b[1;32m    240\u001b[0m scale_tracking_callback_input_layer \u001b[38;5;241m=\u001b[39m ScaleTrackingCallback(quantized_model\u001b[38;5;241m.\u001b[39mget_layer(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    241\u001b[0m scale_tracking_callback_dense_layer \u001b[38;5;241m=\u001b[39m ScaleTrackingCallback(quantized_model\u001b[38;5;241m.\u001b[39mget_layer(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m--> 243\u001b[0m \u001b[43mquantized_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mscale_tracking_callback_input_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale_tracking_callback_dense_layer\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m quantized_model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantized Model Test Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/Thesis/myenv/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "eps_float32 = np.finfo(np.float32).eps\n",
    "\n",
    "class ScaleTrackingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, layer):\n",
    "        super(ScaleTrackingCallback, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.scale_values_per_epoch_w = []\n",
    "        self.average_scale_values_w = []\n",
    "        self.scale_values_per_epoch_b = []\n",
    "        self.average_scale_values_b = []\n",
    "\n",
    "        self.epoch_loss = []\n",
    "        self.epoch_accuracy = []\n",
    "        self.num_epochs = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        scale_values_w = self.layer.scale_w.numpy().flatten()\n",
    "        self.scale_values_per_epoch_w.append(scale_values_w)\n",
    "        average_scale_value_w = np.mean(scale_values_w)\n",
    "        self.average_scale_values_w.append(average_scale_value_w)\n",
    "\n",
    "        scale_values_b = self.layer.scale_b.numpy().flatten()\n",
    "        self.scale_values_per_epoch_b.append(scale_values_b)\n",
    "        average_scale_value_b = np.mean(scale_values_b)\n",
    "        self.average_scale_values_b.append(average_scale_value_b)\n",
    "\n",
    "        self.epoch_loss.append(logs['loss'])\n",
    "        self.epoch_accuracy.append(logs['accuracy'])\n",
    "        print(f\"\\nEpoch {epoch+1}: \\nAverage scale value of w: {average_scale_value_w}, \\nAverage scale value of b: {average_scale_value_b}, \\nLoss: {logs['loss']}, \\nAccuracy: {logs['accuracy']}\")\n",
    "\n",
    "    def plot_scale_values(self, layer_name):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot each scale value trajectory\n",
    "        for i in range(len(self.scale_values_per_epoch_w[0])):\n",
    "            scale_trajectory = [epoch[i] for epoch in self.scale_values_per_epoch_w]\n",
    "            plt.plot(range(1, len(self.scale_values_per_epoch_w) + 1), scale_trajectory, linestyle='-', marker='o')\n",
    "\n",
    "        # Plot the average scale values\n",
    "        #plt.plot(range(1, len(self.average_scale_values) + 1), self.average_scale_values, label='Average Scale Value', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Scale Values')\n",
    "        plt.title('Scale Values of w per Epoch - ' + layer_name)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_w) + 1))  \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Plot each scale value trajectory\n",
    "        for i in range(len(self.scale_values_per_epoch_b[0])):\n",
    "            scale_trajectory = [epoch[i] for epoch in self.scale_values_per_epoch_b]\n",
    "            plt.plot(range(1, len(self.scale_values_per_epoch_b) + 1), scale_trajectory, linestyle='-', marker='o')\n",
    "\n",
    "        # Plot the average scale values\n",
    "        #plt.plot(range(1, len(self.average_scale_values) + 1), self.average_scale_values, label='Average Scale Value', color='red', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Scale Values')\n",
    "        plt.title('Scale Values of b per Epoch - ' + layer_name)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_b) + 1))  \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_loss_accuracy(self, penalty_rate):\n",
    "        epochs = range(1, len(self.epoch_loss) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, self.epoch_loss, 'b-', label='Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss per Epoch - with penalty rate = ' + penalty_rate)\n",
    "        plt.xticks(range(1, len(self.average_scale_values_w) + 1))  \n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, self.epoch_accuracy, 'r-', label='Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy per Epoch')\n",
    "        plt.xticks(range(1, len(self.epoch_accuracy) + 1))  \n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Prepare the data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = np.expand_dims(x_train, -1)  # CNNs typically expect input data to be 4D\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "class LearnedQuantizedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super(LearnedQuantizedDense, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"random_normal\", trainable=True)\n",
    "        self.scale_w = self.add_weight(shape=(1, self.units), initializer=\"ones\", trainable=True)\n",
    "        self.scale_b = self.add_weight(shape=(1,), initializer=\"ones\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        If we use the second option, then it would be that during backpropagation there would be no updates at all.\n",
    "        That's the thing, the backpropagation it would update if it's the part of the loss function. How the gradient is actually being propagated to that? \n",
    "        Because the stop_gradient is there - that's fine - but how is the gradient actually getting pushed through?\n",
    "\n",
    "        - Would the answer to that question not be the fact that during backpropagation the gradient is being calculated on the original value? \n",
    "        - That is the question, right now the gradients are being calculated on the values that are quantized, because that is what you're putting to the tf.matmul,\n",
    "        and then the gradients get pushed further, back through the subsequent layers, but also pushed through that part that you don't have a stop_gradient on the \n",
    "        self.w/self.scale_b. I do not see why you could not get the gradients in a different way. Right now as I see it, I would use the second version and really try to make it work. \n",
    "\n",
    "        The thing is that you can get the gradients in your loss function, but how you would do it is that you do exactly what it says. \n",
    "        self.w/self.scale_w that value that comes out of it, that value you have to return as well. So instead of returning output, you return ....\n",
    "\n",
    "        What we really want to have is that clean quantized_w through the flow and at the same time separately from it, because the plus minus thing gets expensive in the long run.\n",
    "        What I'm pushing through is that you return the self.w/self.scale_w as an output, then you can use that term as something you can calculate the loss from\n",
    "        \"\"\"\n",
    "\n",
    "        quantized_w = tf.stop_gradient(tf.floor(self.w / self.scale_w)) + self.w / self.scale_w - tf.stop_gradient(self.w / self.scale_w)\n",
    "        #tf.stop_gradient(tf.floor(self.w / self.scale_w)) ensures that the quantization operation does not receive gradients during backpropagation.\n",
    "        #self.w / self.scale_w - tf.stop_gradient(self.w / self.scale_w) essentially cancels out the effect of tf.stop_gradient on self.w / self.scale_w,\n",
    "        #  allowing gradients to flow through the original weights divided by the scale\n",
    "\n",
    "        #The entire expression ensures that during the forward pass, the weights are quantized, \n",
    "        # but during the backward pass, the gradients flow through the unquantized path (self.w / self.scale_w), allowing the original weights to be updated correctly.\n",
    "\n",
    "\n",
    "        #quantized_w = tf.stop_gradient(tf.floor(self.w / self.scale_w))\n",
    "\n",
    "        dequantized_w = quantized_w * self.scale_w\n",
    "\n",
    "        quantized_b = tf.stop_gradient(tf.floor(self.b / self.scale_b)) + (self.b / self.scale_b - tf.stop_gradient(self.b / self.scale_b))\n",
    "        #quantized_b = tf.stop_gradient(tf.floor(self.b / self.scale_b))\n",
    "\n",
    "        dequantized_b = quantized_b * self.scale_b\n",
    "\n",
    "        output = tf.matmul(inputs, dequantized_w) + dequantized_b\n",
    "#        output = tf.matmul(inputs, quantized_w) + quantized_b\n",
    "\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output\n",
    "\n",
    "    def get_scale(self):\n",
    "        return self.scale_w, self.scale_b\n",
    "\n",
    "def custom_loss(y_true, y_pred, scales_of_layers, weights_and_biases, penalty_rate):\n",
    "    \"\"\"\n",
    "    Version 1:\n",
    "    do not do the min stuff. \n",
    "\n",
    "    Version 2:\n",
    "    Even simpler, take w and b as they are and you subtract w scaled and b scaled, then the value you get is the difference between quantized and unquantized values.\n",
    "    Okay, wait, this might be wrong because then the gradients will not be continuous.    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    cross_entropy_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # small epsilon to avoid division by zero\n",
    "    epsilon = eps_float32\n",
    "    scale_penalty = 0\n",
    "    \n",
    "    for i, ((scale_w, scale_b), (w, b)) in enumerate(zip(scales_of_layers, weights_and_biases)):\n",
    "        max_w = tf.reduce_max(w, axis=0)\n",
    "        min_w = tf.reduce_min(w, axis=0)\n",
    "        max_b = tf.reduce_max(b)\n",
    "        min_b = tf.reduce_min(b)\n",
    "        \n",
    "        max_w_quantized = (max_w / (scale_w + epsilon))\n",
    "        min_w_quantized = (min_w / (scale_w + epsilon))\n",
    "        max_b_quantized = (max_b / (scale_b + epsilon))\n",
    "        min_b_quantized = (min_b / (scale_b + epsilon))\n",
    "\n",
    "        range_of_quant_w = max_w_quantized - min_w_quantized\n",
    "        range_of_quant_b = max_b_quantized - min_b_quantized\n",
    "\n",
    "        scale_penalty += tf.reduce_mean(tf.abs(range_of_quant_w))\n",
    "        scale_penalty += tf.reduce_mean(tf.abs(range_of_quant_b))\n",
    "    \n",
    "    total_loss = cross_entropy_loss + penalty_rate * scale_penalty\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "flatten_layer = tf.keras.layers.Flatten()(input_layer)\n",
    "quantized_dense_layer1 = LearnedQuantizedDense(128, activation='relu')\n",
    "dense_output1 = quantized_dense_layer1(flatten_layer)\n",
    "quantized_dense_layer2 = LearnedQuantizedDense(10, activation='softmax')\n",
    "output_layer = quantized_dense_layer2(dense_output1)\n",
    "\n",
    "quantized_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Print layer details\n",
    "for i, layer in enumerate(quantized_model.layers):\n",
    "    print(f\"\\nLAYER {i}: {layer}\")\n",
    "    print(f\"  - Input Shape: {layer.input_shape}\")\n",
    "    print(f\"  - Output Shape: {layer.output_shape}\")\n",
    "    if hasattr(layer, 'get_scale'):\n",
    "        print(f\"  - Shape of scale_w: {layer.get_scale()[0].shape}\")\n",
    "        print(f\"  - Shape of scale_b: {layer.get_scale()[1].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "penalty_rates = [0.0, \n",
    "                 #0.1, \n",
    "                 #1.0, \n",
    "                 #10.0\n",
    "                 ]\n",
    "\n",
    "for penalty_rate in penalty_rates:\n",
    "    quantized_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, \n",
    "                                                [quantized_model.get_layer(index=2).get_scale(), \n",
    "                                                quantized_model.get_layer(index=3).get_scale()],\n",
    "                                                [(quantized_model.get_layer(index=2).w, quantized_model.get_layer(index=2).b),\n",
    "                                                (quantized_model.get_layer(index=3).w, quantized_model.get_layer(index=3).b)],\n",
    "                                                penalty_rate=penalty_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    scale_tracking_callback_input_layer = ScaleTrackingCallback(quantized_model.get_layer(index=2))\n",
    "    scale_tracking_callback_dense_layer = ScaleTrackingCallback(quantized_model.get_layer(index=3))\n",
    "\n",
    "    quantized_model.fit(\n",
    "        x_train, y_train,\n",
    "        epochs=20,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[scale_tracking_callback_input_layer,\n",
    "                scale_tracking_callback_dense_layer]\n",
    "    )\n",
    "\n",
    "    loss, accuracy = quantized_model.evaluate(x_test, y_test)\n",
    "    print(f'Quantized Model Test Accuracy: {accuracy}')\n",
    "\n",
    "    scale_tracking_callback_input_layer.plot_scale_values(layer_name=\"Quantized Dense Layer 1\")\n",
    "    scale_tracking_callback_dense_layer.plot_scale_values(layer_name=\"Quantized Dense Layer 2\")\n",
    "    scale_tracking_callback_dense_layer.plot_loss_accuracy(penalty_rate=str(penalty_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
