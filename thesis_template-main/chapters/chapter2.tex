\chapter{Background\label{cha:chapter2}}
This chapter covers the theoretical building blocks related to learned quantization, 
a process whereby the model learns its quantization parameters during training
instead of receiving static quantization requirements as input or undergoing discretization post training. 
Although the primary focus of this thesis exclusively lies on learned quantization, 
a general explanation of the term quantization and its types will be provided in the first section of this chapter
to deliver a bigger picture of this work's relevance in the broader context. 
The following sections will detail the specific terms and methods that are directly associated to the technical setup employed in the experiments, 
starting from the basics of deep neural networks (NNs) and ending with their common modification techniques used in learned quantization scenarios.

\section{Quantization}
\label{sec:section1}
This section aims to answer the \textit{why} question with respect to quantization and further provides a broader understanding of the term regarding its types.

\subsection{Purpose \& Definition}
\label{subsec:subsection1}
With humans becoming increasingly dependent on deep learning models disguised as everyday tools — such as facial recognition filters, 
document scanners, self-driving cars, and more — the need for these models to function in a resource- and 
time-efficient manner is more imperative than ever. Among the many ways to meet this need, 
quantization has become one of the most common techniques used to reduce the computational 
and memory costs of deep NNs, given the sheer amount of parameters they possess and the inherent redundancy this introduces.
\\
\\
While the term \textit{quantization} has its roots in the first half of the 20th century \cite{gray1998quantization}, in the context of NNs, 
it has gained renewed importance  — the over-parameterization of NNs introduces a certain type of flexibility 
that allows quantization to be implemented in many different forms \cite{gholami2021survey}.
Despite the abundance of these forms and approaches, quantization of NNs generally refers to the process of 
reducing the bit precision of their parameters, all while maintaining accuracy within an acceptable range.

\subsection{Quantization Types}
\label{subsec:subsection2}

Although there is a multitude of ways to classify NN quantization methods, the most general classification is the division between \textit{static, dynamic}, and \textit{learned quantization}.
    \begin{itemize}
        \item In \textbf{static quantization}, quantization parameters are fixed before model inference, based on the data observed during training or calibration.
        This corresponds to Post-Training Quantization (PTQ), which directly quantizes the trained floating-point model \cite{jiang2021efficient}, 
        using various discretization approaches based on the range and distribution of the parameters being quantized, 
        as well as the level of granularity at which the quantization values are applied.
        
        \item In \textbf{dynamic quantization}, the quantization parameters are calculated dynamically during inference. 
        This typically applies to activations, as their range changes depending on the input data and, unlike that of the weights, 
        cannot be precomputed with static quantization \cite{kim2021ibert}.
        
        \item In \textbf{learned quantization}, quantization parameters are learned as part of the model training process.
        This corresponds to Quantization Aware Training (QAT) \cite{jacob2018quantization}, where quantization is integrated directly into training rather than applied afterwards.
        Since learned quantization is central to this work, a detailed review of QAT and its applications is provided in the Related Work chapter. 

    \end{itemize}

\section{Fundamentals of Deep Learning}
\label{sec:section2}
With the definition of quantization clarified, this section introduces the fundamental concepts of deep learning necessary for understanding it, 
beginning with simple neural network architectures and progressing to the concepts of forward-pass and back-propagation.

\subsection{Dense Layers in Neural Networks}
\label{subsec:subsection1}
NNs are a mathematical abstraction of the human brain, where nodes pass signals to one another, 
similar to how neurons in the brain communicate. These nodes take an input and generate an output, 
much like answering a question based on given information. Any information can be represented as a set of numeric values — 
for example, an image is a set of pixel values, each representing a specific color. Using this set of values as input, 
a NN can generate an output, possibly identifying what's in the image.
\\
\\
To generate an output from the input, the classical form of NNs, called the Multilayer Perceptron (MLP), 
processes the input through multiple layers of nodes, where each layer applies a transformation to the data. 
These transformations are determined by the \textbf{weights} (\( W \)) and \textbf{biases} (\( b \)) 
associated with the nodes, which are learned during training. At each layer, the operation is defined as:

\[
\text{output} = f(W \cdot \text{input} + b)
\]
\\
\noindent where \( f \) is an activation function applied to the weighted sum. The process repeats across layers until the final output is produced,
such as identifying the content of an image.
\\
\\

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[scale=0.8, every node/.style={transform shape}]

      % Input Layer
      \foreach \i in {1,...,3}
        \node[draw, circle, minimum size=0.8cm] (I-\i) at (0,-2.0*\i) {Input \i};
      
      % Hidden Layer 1
      \foreach \i in {1,...,5}
        \node[draw, circle, minimum size=0.8cm] (H1-\i) at (3,-1.5*\i+0.75) {H1 \i};
      
      % Hidden Layer 2
      \foreach \i in {1,...,5}
        \node[draw, circle, minimum size=0.8cm] (H2-\i) at (6,-1.5*\i+0.75) {H2 \i};
      
      % Output Layer
      \foreach \i in {1,...,2}
        \node[draw, circle, minimum size=0.8cm] (O-\i) at (9,-2.0*\i-0.75) {Output \i};
      
      % Connecting layers
      \foreach \i in {1,...,3}
        \foreach \j in {1,...,5}
          \draw[->] (I-\i) -- (H1-\j);
      
      \foreach \i in {1,...,5}
        \foreach \j in {1,...,5}
          \draw[->] (H1-\i) -- (H2-\j);
      
      \foreach \i in {1,...,5}
        \foreach \j in {1,...,2}
          \draw[->] (H2-\i) -- (O-\j);
      
  \end{tikzpicture}
  \caption{Multilayer Perceptron with Two Hidden Layers}
  \label{fig:mlp_diagram}
\end{figure}


\noindent The layers in between, where this operation is applied, are called hidden layers. 
They are typically dense layers, meaning each node in one layer is connected to every node in the next. 
Dense layers introduce the inherent redundancy of NNs due to the large number of numeric values they contain, 
particularly in \( W \) and \( b \). Thus, modifying the operations these layers perform 
and the parameters they learn will be a key focus of this work.

\subsection{Loss Functions \& Regularization}
\label{subsec:subsection2}
The training process of NNs is similar to how our brains learn from mistakes. 
Given the ground truth, an MLP, for example, adjusts its learnable parameters in \( W \) and \( b \)
using a specific function that compares the ground truth with the output generated by the MLP.
\\
\\
This function is called a \textit{loss function}, and depending on the type of question the network aims to answer, it can take many different forms.
For example, for the MLP described in Figure~\ref{fig:mlp_diagram} that generates a binary classification, we would use the \textit{log loss} function. 
Since the datasets used in this thesis involve multi-class classification, the \textit{sparse categorical cross-entropy} (SCCE) loss function will be used, 
which measures the difference between the predicted class probabilities and the true labels for each class in the dataset.
\\
\\
Often the loss function alone is not enough for the NN to perform well. This is why a regularization term that penalizes unwanted behaviours
is added to the loss function.
\\
\\
An example approach is to define a regularization term that directly penalizes large differences between full-precision values 
and their quantized counterparts \cite{zhuang2018towards}. This can encourage the model to learn parameters that are easily quantizable without significant performance loss.
If the values that are being quantized are \( W \), then this regularization term could look like:

\[
\mathcal{L}_{\text{quant}} = \lambda \sum_{i} \| W_i - q(W_i) \|^2
\]

\noindent Where:
\begin{itemize}
    \item \( \lambda \) is a scalar that controls the importance of the quantization penalty.
    \item \( W_i \) represents the full-precision weight value before quantization at index \( i \).
    \item \( q(W_i) \) represents the quantized version of \( W_i \).
\end{itemize}

\noindent The current work uses multiple custom regularization terms that trigger the quantization process during training. 
These will be discussed in detail in the Experimental Setup section.

\subsection{Forward-Pass \& Back-Propagation}
\label{subsec:subsection2}
The repetition of the operation described earlier in the Dense Layers in Neural Networks subsection during model training 
is essentially the forward pass. It is the process where input data is passed through the network layer by layer, 
with each layer applying its learned weights and biases to produce a final output. 
\\
\\
As mentioned in the previous subsection, this output is then compared with the ground truth by the loss function that produces an error.
This error is used to update the learnable parameters — in case of MLPs, the values in \( W \) and \( b \) — during a process called back-propagation.
\\
\\
In other words, back-propagation is the method by which the network adjusts its parameters to minimize the error. 
It calculates the gradient of the loss function with respect to each parameter using the chain rule. 
\( W \) and \( b \) are typically updated as follows:

\[
W = W - \eta \frac{\partial L}{\partial W}, \quad b = b - \eta \frac{\partial L}{\partial b}
\]
\\
\noindent where \( L \) is the loss function, and \( \eta \) is the learning rate.
\\
\\
For example, consider the weight \( W_{I1,H1_1} \) represented as the line between \textit{Input 1} and the hidden layer node \( H1_1 \) in Figure~\ref{fig:mlp_diagram}. 
The gradient of this weight with respect to the loss is calculated using the chain rule:

\[
\frac{\partial L}{\partial W_{I1,H1_1}} = \frac{\partial L}{\partial \text{Output 1}} \cdot \frac{\partial \text{Output 1}}{\partial H1_1} \cdot \frac{\partial H1_1}{\partial W_{I1,H1_1}}
\]

\noindent Where:
\begin{itemize}
    \item \( \frac{\partial L}{\partial \text{Output 1}} \) is the gradient of the loss with respect to \textit{Output 1}.
    \item \( \frac{\partial \text{Output 1}}{\partial H1_1} \) is the gradient of \textit{Output 1} with respect to the output of \( H1_1 \).
    \item \( \frac{\partial H1_1}{\partial W_{I1,H11}} \) is the value of \textit{Input 1}, since \( H1_1 \) is a weighted sum of the inputs.
\end{itemize}

\noindent This shows how each weight contributes to the final error during back-propagation.


\section{Common Concepts of QAT}
\label{sec:section2}
Now that the fundamentals of deep learning have been covered, 
this section introduces concepts commonly encountered in learned quantization, 
including the modified forward-pass technique and the challenges it creates for back-propagation.
\subsection{Low-Precision Forward-Pass}
\label{subsec:subsection1}
In QAT, the operation performed during the forward-pass is often modified. Instead of propagating the full precision values, 
the output of a layer is first quantized and then dequantized before being passed to the next layer \cite{jacob2018quantization}. 
This simulates the effect of quantization during inference, helping the network adjust to the reduced precision parameters.
\\
\\
The \textit{quantization} process can be generalized as follows:

\[
q(x) = \text{round}\left( \frac{x}{P} \right) - Z
\]
\\
\noindent where \( q(x) \) is the quantized value, \( x \) is the full precision value of the parameter that is being quantized, \( P \) is a quantization parameter, such as a scaling factor, 
and \( Z \) represents the zero point. This method is the most commonly used \textit{uniform quantization}, which can be either \textit{asymmetric} or \textit{symmetric}, 
depending on the value of \( Z \), with  \( Z \) = 0 representing \textit{symmetric quantization}.
\\
\\
For \textit{dequantization}, we apply:

\[
x_{\text{dequant}} = P \cdot ( q(x) + Z)
\]
\\
\noindent Note that the dequantized value does not necessarily match the initial value. 
For example, if \( x = 7.8 \) and \( P = 2 \), then quantization gives \( q(x) = 4 \), and dequantization results in \( x_{\text{dequant}} = 8 \). 
This demonstrates how the \textit{quantization} and dequantization process approximates the original value \( x \)
by rounding it to the nearest representable value based on the quantization parameter \( P \).

\subsection{Full-Precision Back-Propagation}
\label{subsec:subsection1}
In QAT, although the forward-pass operates on quantized values, back-propagation typically uses full-precision values. 
This is because the rounding operation in the forward-pass is non-differentiable, which makes it impossible to compute gradients directly.
\\
\\
Consider the example of computing the gradient of the weight \( W_{I1,H1_1} \) in Figure~\ref{fig:mlp_diagram}:

\[
\frac{\partial L}{\partial W_{I1,H1_1}} = \frac{\partial L}{\partial \text{Output 1}} \cdot \frac{\partial \text{Output 1}}{\partial H1_1} \cdot \frac{\partial H1_1}{\partial W_{I1,H1_1}}
\]
\\
\noindent If the output of \( H1_1 \) was quantized using a rounding operation, the gradient 
\[
\frac{\partial H1_1}{\partial W_{I1,H1_1}} 
\]
would be undefined due to the non-differentiability of rounding. This would break the gradient flow, making it impossible to update  \( W_{I1,H1_1} \)
during back-propagation.
\\
\\
To work around this, techniques such as the Straight-Through Estimator (STE) are commonly used \cite{bengio2013estimating} \cite{fan2021training} \cite{zhuang2018towards}.
The STE approximates the gradient by treating the rounding function as if it were differentiable during the back-propagation.
This approach ensures that the model can simulate low-precision inference during the forward pass,
while still benefiting from the accuracy of full-precision gradient updates during back-propagation.
\\
\\
The Experimental Setup section will explain a slighly different version of the typical STE that was used in the current work.